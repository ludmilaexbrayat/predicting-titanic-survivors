{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<br>\n",
    "**Context**\n",
    "<br>\n",
    "This notebook was created to participate in the Kaggle competition \"[Titanic: Machine Learning fro Disaster](https://www.kaggle.com/c/titanic)\".\n",
    "<br>\n",
    "\n",
    "**Objective**\n",
    "<br>\n",
    "The goal of the competition is to predict which passengers will survive the tragedy.\n",
    "<br>\n",
    "\n",
    "**Results**\n",
    "<br>\n",
    "On August of 2018, I got an accuracy score of 77%.\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature exploration, engineering and cleaning\n",
    "<br>\n",
    "First, let's set up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# Ignore warning messages\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn\n",
    "\n",
    "# Handle table-like data and matrices\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Perform computations\n",
    "from math import floor\n",
    "\n",
    "# Modelling Algorithms\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, ExtraTreesClassifier, AdaBoostClassifier\n",
    "from sklearn.grid_search import ParameterGrid\n",
    "import xgboost\n",
    "\n",
    "# Modelling Helpers\n",
    "from sklearn.preprocessing import Imputer , Normalizer , scale\n",
    "from sklearn.cross_validation import train_test_split , StratifiedKFold, KFold\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Visualization\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# No warning\n",
    "pd.options.mode.chained_assignment = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_uuid": "1a74a30c1106761ac83ed01e9c61f29c22ad38a5"
   },
   "outputs": [],
   "source": [
    "# Upload datasets\n",
    "train = pd.read_csv('train.csv')\n",
    "train_passenger_ids = train.PassengerId.tolist()\n",
    "\n",
    "test = pd.read_csv('test.csv')\n",
    "test_passenger_ids = test.PassengerId.tolist()\n",
    "\n",
    "all_data = pd.concat([train, test], sort=False, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "84a61c30ccb69c99c9dde62c6e82142d6760314f"
   },
   "source": [
    "## 1. Creating new features\n",
    "<br>\n",
    "### A. From the _Cabin_ column ...\n",
    "<br>... we are going to extract:\n",
    "- <u>has_cabin_number</u> 1 if the traveller had a cabin number.\n",
    "- <u>cabin_deck</u>: deck where the cabin was located.\n",
    "- <u>cabin_room_number</u>: room number of the cabin.\n",
    "- <u>cabin_type</u>: \"Suite\" if the traveller had several rooms.\n",
    "- <u>cabin_weird_number</u>: 1 if the cabin number is like \"F H30\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_uuid": "b74f70e83fc0aeeb04a1cd2cb5af662c00cd2fb3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Share of travellers without a cabin number, broken down by Pclass:\n",
      "Pclass\n",
      "1    0.207430\n",
      "2    0.916968\n",
      "3    0.977433\n",
      "Name: PassengerId, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# UNDERSTANDING NAN\n",
    "print(\"Share of travellers without a cabin number, broken down by Pclass:\")\n",
    "print(all_data[all_data.Cabin.isna()].groupby(['Pclass']).count().PassengerId/all_data.groupby(['Pclass']).count().PassengerId)\n",
    "## We can't deduct anything from a NaN in this column. Some 1st class travellers don't have any Cabin number, and some 3rd class do.\n",
    "\n",
    "all_data[\"has_cabin_number\"] = all_data.Cabin.notnull()*1\n",
    "\n",
    "\n",
    "# CREATING NEW FEATURES\n",
    "rows_with_cabin = all_data[all_data.Cabin.notnull()].index\n",
    "\n",
    "## 1. Extracting the deck\n",
    "all_data[\"cabin_deck\"] = np.nan\n",
    "for index in rows_with_cabin:\n",
    "    deck = all_data.Cabin[index][:1]\n",
    "    if len(deck) == 0:\n",
    "        all_data[\"cabin_deck\"][index] = \"Unknown\"\n",
    "    else:\n",
    "        all_data[\"cabin_deck\"][index] = deck\n",
    "\n",
    "## 2. Extracting the room number\n",
    "#   NB: If multiple cabins, we take the number of the last room:\n",
    "all_data[\"cabin_room_number\"] = np.nan\n",
    "for index in rows_with_cabin:\n",
    "    # Rooms in the deck (Cabin = T) didn't have any number\n",
    "    if len(all_data.Cabin[index])==1:\n",
    "        all_data[\"cabin_room_number\"][index] = np.nan\n",
    "\n",
    "    # We take the last character of the string if ...    \n",
    "    elif len(all_data.Cabin[index])==2:\n",
    "        raw_number = int(all_data[\"Cabin\"][index][-1:])\n",
    "        lower_boundary = str(50 * floor(raw_number/50))\n",
    "        upper_boundary = str(50 * (floor(raw_number/50) +1))\n",
    "        all_data[\"cabin_room_number\"][index] = lower_boundary + \" - \" + upper_boundary\n",
    "    \n",
    "    # We take the last 2 characters of the string if ...\n",
    "    elif (len(all_data.Cabin[index])==3\n",
    "        or (len(all_data.Cabin[index]) in (5,7) and all_data.Cabin[index].count(' ')==1)\n",
    "        or (len(all_data.Cabin[index])==11 and all_data.Cabin[index].count(' ')==2)\n",
    "        or len(all_data.Cabin[index])>11\n",
    "       ):\n",
    "        raw_number = int(all_data[\"Cabin\"][index][-2:])\n",
    "        lower_boundary = str(50 * floor(raw_number/50))\n",
    "        upper_boundary = str(50 * (floor(raw_number/50) +1))\n",
    "        all_data[\"cabin_room_number\"][index] = lower_boundary + \" - \" + upper_boundary\n",
    "    # We take the last 3 characters of the string if ...\n",
    "    elif (len(all_data.Cabin[index])==4\n",
    "          or (all_data.Cabin[index].count(' ')==1 and len(all_data.Cabin[index])==6)\n",
    "          or (all_data.Cabin[index].count(' ')==1 and len(all_data.Cabin[index])>7)\n",
    "          or (all_data.Cabin[index].count(' ')==2 and len(all_data.Cabin[index])>11)\n",
    "         ):\n",
    "        raw_number =  int(all_data[\"Cabin\"][index][-3:])\n",
    "        lower_boundary = str(50 * floor(raw_number/50))\n",
    "        upper_boundary = str(50 * (floor(raw_number/50) +1))\n",
    "        all_data[\"cabin_room_number\"][index] = lower_boundary + \" - \" + upper_boundary\n",
    "    # Let's check that we're not missing any case scenario\n",
    "    else: \n",
    "        all_data[\"cabin_room_number\"][index] = 'Catch me if you can'\n",
    "\n",
    "\n",
    "## 3. Extracting the number of cabins\n",
    "all_data[\"cabin_type\"] = np.nan\n",
    "for index in rows_with_cabin:\n",
    "    if (all_data.Cabin[index].count(' ')>1) or (all_data.Cabin[index].count(' ')==1 and len(all_data.Cabin[index])>5):\n",
    "        all_data[\"cabin_type\"][index] = \"Suite\"\n",
    "    else:\n",
    "        all_data[\"cabin_type\"][index] = \"Standard\"\n",
    "\n",
    "\n",
    "## 4. Isolating weird cases\n",
    "all_data[\"cabin_weird_number\"] = 0\n",
    "for index in rows_with_cabin:\n",
    "    if all_data.Cabin[index].count(' ')==1 and len(all_data.Cabin[index])==5:\n",
    "        all_data[\"cabin_weird_number\"][index] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f45a10d3fa8322a01a855083de2f2324a3c338b6"
   },
   "source": [
    "### B. From the _Ticket_ column ...\n",
    "<br>... we are going to extract:\n",
    "- <u>ticket_prefix</u>: if exist, the letters at the beginning of the ticket number.\n",
    "- <u>ticket_number</u>: if the ticket has a prefix, it gives the numbers after the prefix. Otherwise, it equals the existing Ticket feature.\n",
    "- <u>ticket_n_pax</u>: number of travellers with the same ticket.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_uuid": "8a50b9ed344bcbd675d110744fb52a0dd5f1b66b"
   },
   "outputs": [],
   "source": [
    "rows_with_ticket = all_data[all_data.Ticket.notnull()].index\n",
    "\n",
    "## 1. Extracting ticket prefix\n",
    "all_data[\"ticket_prefix\"] = np.nan\n",
    "\n",
    "for index in rows_with_ticket:\n",
    "    if all_data.Ticket[index].find(\" \") != -1:\n",
    "        all_data[\"ticket_prefix\"][index] = all_data.Ticket[index][:all_data.Ticket[index].find(\" \")]\n",
    "    else:\n",
    "        all_data[\"ticket_prefix\"][index] = \"No prefix\"\n",
    "\n",
    "\n",
    "## 2. Extracing ticket number\n",
    "all_data[\"ticket_number\"] = np.nan\n",
    "\n",
    "for index in rows_with_ticket:\n",
    "    all_data[\"ticket_number\"][index] = all_data.Ticket[index][-(len(all_data.Ticket[index])-all_data.Ticket[index].find(\" \")):]\n",
    "\n",
    "\n",
    "## 3. Computing the number of travellers with same ticket\n",
    "n_pax = all_data.groupby(\"Ticket\").count().PassengerId.to_frame()\n",
    "n_pax.columns = ['ticket_n_pax']\n",
    "all_data = all_data.merge(n_pax, how = \"left\", on = \"Ticket\", suffixes=('', '_y'), copy = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d7bbb55337114c726abaee22072756d409ba3a1e"
   },
   "source": [
    "### C. From the _Name_ column ...\n",
    "... we are going to extract:\n",
    "- <u>name_title</u>: the title of the traveller.\n",
    "- <u>has_nickname</u>: 1 if a nickname is specified.\n",
    "- <u>has_maiden_name</u>: 1 if a maiden name is specified.\n",
    "- <u>has_multiple_last_names</u>: 1 if the traveller has several last names.\n",
    "- <u>has_multiple_first_names</u>: 1 if the traveller has several first names.\n",
    "- <u>name_has_particle</u>: 1 if the traveller has a particle in his last name.\n",
    "- <u>name_has_dash</u>: 1 if the traveller has a \"-\" in his name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_uuid": "0bc422ddf7114177e67ba129daa33f1d755ef750"
   },
   "outputs": [],
   "source": [
    "# 1. Extracting the title\n",
    "all_data[\"name_title\"] = np.nan\n",
    "for index in range(0,len(all_data)):\n",
    "    # The title is after the comma, and before the point\n",
    "    all_data[\"name_title\"][index] = all_data.Name[index][all_data.Name[index].find(',')+2:all_data.Name[index].find('.')]\n",
    "\n",
    "# 2. Extracting if there is a nickname (\")\n",
    "all_data[\"has_nickname\"] = 0\n",
    "for index in range(0,len(all_data)):\n",
    "    # A nickname is separated by quotation marks\n",
    "    if '\"' in all_data.Name[index]:\n",
    "        all_data[\"has_nickname\"][index] = 1\n",
    "\n",
    "# 3. Extracting if there is a maiden name\n",
    "all_data[\"has_maiden_name\"] = np.nan\n",
    "for index in range(0,len(all_data)):\n",
    "    if '(' in all_data.Name[index] and all_data.Sex[index]==\"female\":\n",
    "        all_data[\"has_maiden_name\"][index] = 1\n",
    "    elif all_data.Sex[index] == \"female\":\n",
    "        all_data[\"has_maiden_name\"][index] = 0\n",
    "\n",
    "# 4. Extracting the last name(s)\n",
    "all_data[\"last_name\"] = np.nan\n",
    "for index in range(0,len(all_data)):\n",
    "    all_data[\"last_name\"][index] = all_data.Name[index][:all_data.Name[index].find(\",\")]\n",
    "\n",
    "# 5. Extracting the number of last names\n",
    "all_data[\"n_last_names\"] = np.nan\n",
    "for index in range(0,len(all_data)):\n",
    "    all_data[\"n_last_names\"][index] = all_data.Name[index][:all_data.Name[index].find(\",\")].count(\" \")\n",
    "\n",
    "# 6. Extracting has_multiple_first_names\n",
    "all_data[\"has_multiple_last_names\"] = np.nan\n",
    "for index in range(0,len(all_data)):\n",
    "    if all_data.n_last_names[index] > 1:\n",
    "        all_data[\"has_multiple_last_names\"][index] = 1\n",
    "    else:\n",
    "        all_data[\"has_multiple_last_names\"][index] = 0\n",
    "\n",
    "# 7. Extracting the first name(s)\n",
    "all_data[\"first_name\"] = np.nan\n",
    "for index in range(0,len(all_data)):\n",
    "    if \"(\" in all_data.Name[index] and '\"' in all_data.Name[index]:\n",
    "        position = min(all_data.Name[index].find(\"(\"), all_data.Name[index].find('\"'))\n",
    "        all_data[\"first_name\"][index] = all_data.Name[index][all_data.Name[index].find(\".\")+2:position]\n",
    "\n",
    "    elif \"(\" in all_data.Name[index]:\n",
    "        position = all_data.Name[index].find(\"(\")\n",
    "        all_data[\"first_name\"][index] = all_data.Name[index][all_data.Name[index].find(\".\")+2:position]\n",
    "\n",
    "    elif '\"' in all_data.Name[index]:\n",
    "        position = all_data.Name[index].find('\"')\n",
    "        all_data[\"first_name\"][index] = all_data.Name[index][all_data.Name[index].find(\".\")+2:position]\n",
    "\n",
    "    else:\n",
    "        all_data[\"first_name\"][index] = all_data.Name[index][all_data.Name[index].find(\".\")+2:]\n",
    "\n",
    "# 8. Extracting the number of first names\n",
    "all_data[\"n_first_names\"] = np.nan\n",
    "for index in range(0,len(all_data)):\n",
    "    if \"(\" in all_data.Name[index] and '\"' in all_data.Name[index]:\n",
    "        position = min(all_data.Name[index].find(\"(\"), all_data.Name[index].find('\"'))\n",
    "        all_data[\"n_first_names\"][index] = all_data.Name[index][all_data.Name[index].find(\".\")+2:position].count(\" \")\n",
    "\n",
    "    elif \"(\" in all_data.Name[index]:\n",
    "        position = all_data.Name[index].find(\"(\")\n",
    "        all_data[\"n_first_names\"][index] = all_data.Name[index][all_data.Name[index].find(\".\")+2:position].count(\" \")\n",
    "\n",
    "    elif '\"' in all_data.Name[index]:\n",
    "        position = all_data.Name[index].find('\"')\n",
    "        all_data[\"n_first_names\"][index] = all_data.Name[index][all_data.Name[index].find(\".\")+2:position].count(\" \")\n",
    "\n",
    "    else:\n",
    "        all_data[\"first_name\"][index] = all_data.Name[index][all_data.Name[index].find(\".\")+2:].count(\" \")\n",
    "\n",
    "# 9. Extracting has_multiple_first_names\n",
    "all_data[\"has_multiple_first_names\"] = np.nan\n",
    "for index in range(0,len(all_data)):\n",
    "    if all_data.n_first_names[index] > 1:\n",
    "        all_data[\"has_multiple_first_names\"][index] = 1\n",
    "    else:\n",
    "        all_data[\"has_multiple_first_names\"][index] = 0\n",
    "\n",
    "# 10. Extracting whether there is a particle in the name\n",
    "all_data[\"name_has_particle\"] = 0\n",
    "for index in range(0,len(all_data)):\n",
    "    if all_data.Name[index][:2] == \"de\":\n",
    "        all_data[\"name_has_particle\"][index] = 1\n",
    "\n",
    "# 11. Extracting whethere there is a dash in the name\n",
    "all_data[\"name_has_dash\"] = 0\n",
    "for index in range(0,len(all_data)):\n",
    "    if \"-\" in all_data.Name[index]:\n",
    "        all_data[\"name_has_dash\"][index] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "7a2298e733f9bb23ef3a88a2ee247968194c8ad1"
   },
   "source": [
    "### D. From the _Age_ column ...\n",
    "<br>Let's simply group ages and replace missing values with \"unknown\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_uuid": "9ef3ca50519a3f96844bdab89825ebb2534aba25"
   },
   "outputs": [],
   "source": [
    "all_data[\"age_group\"] = 'unknown'\n",
    "rows_with_age = all_data[all_data.Age.notnull()].index\n",
    "\n",
    "for index in rows_with_age:\n",
    "    lower_boundary = str(10 * floor(all_data.Age[index]/10))\n",
    "    upper_boundary = str(10 * (floor(all_data.Age[index]/10)+1))\n",
    "    all_data[\"age_group\"][index] = lower_boundary + \" - \" + upper_boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "afc76f501efb019dd173693757872e8b2542ab55"
   },
   "source": [
    "### E. From _SibSp_ and _Parch_ columns ...\n",
    "<br>...we are going to extract:\n",
    "- <u>sibsp_group</u>: a grouped version of SibSp.\n",
    "- <u>parch_group</u>: a grouped version of Parch.\n",
    "- <u>relatives_group</u>: the number of relatives (SibSp + Parch).\n",
    "- <u>is_alone</u>: 1 if the traveller doesn't have any relative."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_uuid": "312fa6a41d35a4a7569738ed6640f7186c051e74"
   },
   "outputs": [],
   "source": [
    "# 1. Grouping SibSp\n",
    "all_data[\"sibsp_group\"] = all_data.SibSp\n",
    "for index in range(0,len(all_data)):\n",
    "    if all_data.SibSp[index] > 4:\n",
    "        all_data[\"sibsp_group\"][index] = \"5+\"\n",
    "\n",
    "# 2. Grouping Parch\n",
    "all_data[\"parch_group\"] = all_data.Parch\n",
    "for index in range(0,len(all_data)):\n",
    "    if all_data.Parch[index] > 2:\n",
    "        all_data[\"parch_group\"][index] = \"3+\"\n",
    "\n",
    "# 3. Computing total number of related\n",
    "all_data[\"n_relatives\"] = all_data.SibSp + all_data.Parch\n",
    "\n",
    "# 4. Grouping relatives\n",
    "all_data[\"relatives_group\"] = all_data.n_relatives\n",
    "for index in range(0,len(all_data)):\n",
    "    if all_data.n_relatives[index] > 4:\n",
    "        all_data[\"relatives_group\"][index] = \"5+\"\n",
    "\n",
    "# 5. Creating is_alone\n",
    "all_data[\"is_alone\"] = 0\n",
    "for index in range(0,len(all_data)):\n",
    "    if all_data.n_relatives[index] == 0:\n",
    "        all_data[\"is_alone\"][index] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "960311815cfac7b5bdcb89f8762c03e5161ee943"
   },
   "source": [
    "### F. From the _Fare_ column ...\n",
    "<br>Let's simply group fares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_uuid": "9a6416a7c6aaf9fe0e9bd885100eb4176d780e51"
   },
   "outputs": [],
   "source": [
    "rows_with_fare = all_data[all_data.Fare.notnull()].index\n",
    "\n",
    "all_data[\"fare_group\"] = np.nan\n",
    "\n",
    "for index in rows_with_fare:\n",
    "    if all_data.Fare[index] == 0:\n",
    "        all_data[\"fare_group\"][index] = \"invited\"\n",
    "        \n",
    "    elif all_data.Fare[index] >= 100:\n",
    "        all_data[\"fare_group\"][index] = \"100+\"\n",
    "    \n",
    "    elif all_data.Fare[index] >= 40:\n",
    "        lower_boundary = str(20 * floor(all_data.Fare[index]/20))\n",
    "        upper_boundary = str(20 * (floor(all_data.Fare[index]/20)+1))\n",
    "        all_data[\"fare_group\"][index] = lower_boundary + \" - \" + upper_boundary\n",
    "        \n",
    "    elif all_data.Fare[index] >= 15:\n",
    "        lower_boundary = str(5 * floor(all_data.Fare[index]/5))\n",
    "        upper_boundary = str(5 * (floor(all_data.Fare[index]/5)+1))\n",
    "        all_data[\"fare_group\"][index] = lower_boundary + \" - \" + upper_boundary\n",
    "\n",
    "    else:\n",
    "        lower_boundary = str(2 * floor(all_data.Fare[index]/2))\n",
    "        upper_boundary = str(2 * (floor(all_data.Fare[index]/2)+1))\n",
    "        all_data[\"fare_group\"][index] = lower_boundary + \" - \" + upper_boundary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "1fe9e75ac417792a95db32827f3b6ccb6d685f47"
   },
   "source": [
    "## 2. Selecting features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_uuid": "6635b72b203905b700eeb0e7ed7e54bb38919d90"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 features were dropped.\n"
     ]
    }
   ],
   "source": [
    "cols_to_drop = [\"Name\", \"Age\", \"SibSp\", \"Parch\", \"Ticket\", \"Fare\", \"Cabin\", \"ticket_number\", \"last_name\", \"n_last_names\", \"first_name\", \"n_first_names\", \"n_relatives\"]\n",
    "all_data = all_data.drop(cols_to_drop, axis = 1)\n",
    "print(\"%d features were dropped.\" %(len(cols_to_drop)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "73068de05a5917d36f126a68ace480772d6a5421"
   },
   "source": [
    "## 3. Transforming categorical features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_uuid": "79a4276741688689b38c659d3f8b957a7a1b803a",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We now have 161 features.\n"
     ]
    }
   ],
   "source": [
    "cols_not_to_encode = [\"PassengerId\", \"Survived\"]\n",
    "cols_to_encode = list(set(all_data.columns.tolist())-set(cols_not_to_encode))\n",
    "\n",
    "all_data = pd.get_dummies(all_data, columns=cols_to_encode)\n",
    "print(\"We now have %d features.\" %(len(all_data.columns.tolist())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Splitting train and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = all_data[all_data.PassengerId.isin(train_passenger_ids)]\n",
    "test = all_data[all_data.PassengerId.isin(test_passenger_ids)].drop(\"Survived\", axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "f53723b4305c35677810096b0ab14abcecdbbfaa"
   },
   "source": [
    "# Generating our Base First-Level Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating X and y\n",
    "X = train.drop(\"Survived\", axis = 1)\n",
    "y = train.Survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Defining base parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_grid = {'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n",
    "           'warm_start': [True], \n",
    "           'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n",
    "           'min_samples_leaf': [1, 2, 4],\n",
    "           'min_samples_split': [2, 5, 10],\n",
    "           'max_features': ['auto', 'sqrt']}\n",
    "\n",
    "# Extra Trees\n",
    "et_grid = {'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n",
    "           'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n",
    "           'min_samples_leaf': [1, 2, 4],\n",
    "           'min_samples_split': [2, 5, 10],\n",
    "           'max_features': ['auto', 'sqrt']}\n",
    "\n",
    "# AdaBoost\n",
    "ab_grid = {'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n",
    "           'learning_rate': [x for x in np.arange(0.5, 1.5, 0.1)],\n",
    "           }\n",
    "\n",
    "# GradientBoost\n",
    "gb_grid = {'n_estimators': [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)],\n",
    "           'learning_rate': [x for x in np.arange(0.5, 1.5, 0.1)], \n",
    "           'max_depth': [int(x) for x in np.linspace(10, 110, num = 11)],\n",
    "           'min_samples_leaf': [1, 2, 4],\n",
    "           'min_samples_split': [2, 5, 10],\n",
    "           'max_features': ['auto', 'sqrt']}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Tuning parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "4dc43f4c251de626c95292d44e069e390113a8a7"
   },
   "source": [
    "**Let's get an idea of the parameters we want to use.**\n",
    "<br>As I don't have any idea of the range I should use to test each parameter, I am first going to test randomly 100 combinations of parameters, and save the best one.\n",
    "<br>Inspiration: [Hyperparameter Tuning The Radom Forest in Python](https://towardsdatascience.com/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74), by William Koehrsen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_uuid": "d0423306177ffa6a82121857c8e7ad0616731537"
   },
   "outputs": [],
   "source": [
    "# Defining a function that randomly searches best parameters across\n",
    "# 100 different combinations, using 3 fold cross validation.\n",
    "\n",
    "def get_param_with_randomsearch(model, param_grid, X, y):\n",
    "    \n",
    "    # 1. Fixing parameters\n",
    "    \n",
    "    print(\"RandomizedSearchCV is going to test the following parameters:\")\n",
    "    print(param_grid)\n",
    "    \n",
    "    random_model = RandomizedSearchCV( estimator = model, \n",
    "                                       param_distributions = param_grid,\n",
    "                                       n_iter = 100,\n",
    "                                       cv = 3,\n",
    "                                       random_state = 42,\n",
    "                                       n_jobs = -1,\n",
    "                                       scoring = \"accuracy\")\n",
    "    \n",
    "    \n",
    "    # 2. Fitting the random search model\n",
    "    \n",
    "    random_model.fit(X,y)\n",
    "\n",
    "    print(\"\")\n",
    "    print(\"The best combination of parameters is:\")\n",
    "    print(random_model.best_params_)\n",
    "    print(\"\")\n",
    "    print(\"The best combination of parameters we've found generated an accuracy score of {:0.2f}%.\" .format(random_model.best_score_*100))\n",
    "    \n",
    "    return random_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Now that we have a clearer idea of the kind of parameters we need, let's tune them more precisely using GridSearchCV.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function that \n",
    "#  1. Creates a grid of parameters to test, based on the best parameter set found with get_param_with_randomsearch.\n",
    "#  2. Find the optimal combination of parameters in this grid.\n",
    "\n",
    "def get_param_with_gridsearchcv(model, shortcut, random_param_grid, X, y):\n",
    "    \n",
    "    # 1. Building param grid based on the result of the function get_param_with_randomsearch\n",
    "\n",
    "    param_grid = {}\n",
    "    \n",
    "    stage = \"go\"\n",
    "    \n",
    "    model_str = str(model)\n",
    "    \n",
    "    if shortcut == \"rf\":\n",
    "        param_grid[\"n_estimators\"] = [random_param_grid[\"n_estimators\"]]\n",
    "        param_grid[\"warm_start\"] = [random_param_grid[\"warm_start\"]]\n",
    "        param_grid[\"max_depth\"] = list(set([x for x in range(random_param_grid[\"max_depth\"],random_param_grid[\"max_depth\"]+6,3)] + [x for x in range(random_param_grid[\"max_depth\"],random_param_grid[\"max_depth\"]-6,-3)]))\n",
    "        param_grid[\"min_samples_leaf\"] = list(set([x for x in range(random_param_grid[\"min_samples_leaf\"], random_param_grid[\"min_samples_leaf\"]+3,1)] + [x for x in range(random_param_grid[\"min_samples_leaf\"], max(0,random_param_grid[\"min_samples_leaf\"]-3),-1)]))\n",
    "        param_grid[\"min_samples_split\"] = list(set([x for x in range(random_param_grid[\"min_samples_split\"], random_param_grid[\"min_samples_split\"]+4, 2)] + [x for x in range(random_param_grid[\"min_samples_split\"], max(1,random_param_grid[\"min_samples_split\"]-4), -2)]))\n",
    "        param_grid[\"max_features\"] = [random_param_grid[\"max_features\"]]\n",
    "    \n",
    "\n",
    "    elif shortcut == \"et\":\n",
    "        param_grid[\"n_estimators\"] = [random_param_grid[\"n_estimators\"]]\n",
    "        param_grid[\"max_depth\"] = list(set([x for x in range(random_param_grid[\"max_depth\"],random_param_grid[\"max_depth\"]+6,3)] + [x for x in range(random_param_grid[\"max_depth\"],random_param_grid[\"max_depth\"]-6,-3)]))\n",
    "        param_grid[\"min_samples_leaf\"] = list(set([x for x in range(random_param_grid[\"min_samples_leaf\"], random_param_grid[\"min_samples_leaf\"]+3,1)] + [x for x in range(random_param_grid[\"min_samples_leaf\"], max(0,random_param_grid[\"min_samples_leaf\"]-3),-1)]))\n",
    "        param_grid[\"max_features\"] = [random_param_grid[\"max_features\"]]\n",
    "        param_grid[\"min_samples_split\"] = list(set([x for x in range(random_param_grid[\"min_samples_split\"], random_param_grid[\"min_samples_split\"]+4, 2)] + [x for x in range(random_param_grid[\"min_samples_split\"], max(1,random_param_grid[\"min_samples_split\"]-4), -2)]))\n",
    "    \n",
    "    elif shortcut == \"ab\":\n",
    "        param_grid[\"n_estimators\"] = [random_param_grid[\"n_estimators\"]]\n",
    "        param_grid[\"learning_rate\"] = list(set([x for x in np.arange(random_param_grid[\"learning_rate\"],max(0,random_param_grid[\"learning_rate\"]-0.5),-0.02)]+[x for x in np.arange(random_param_grid[\"learning_rate\"],max(0,random_param_grid[\"learning_rate\"]+0.5),0.02)]))\n",
    "\n",
    "    elif shortcut == \"gb\":\n",
    "        param_grid[\"n_estimators\"] = [random_param_grid[\"n_estimators\"]]\n",
    "        param_grid[\"max_depth\"] = list(set([x for x in range(random_param_grid[\"max_depth\"],random_param_grid[\"max_depth\"]+4,3)] + [x for x in range(random_param_grid[\"max_depth\"],random_param_grid[\"max_depth\"]-4,-3)]))\n",
    "        param_grid[\"min_samples_leaf\"] = list(set([x for x in range(random_param_grid[\"min_samples_leaf\"], random_param_grid[\"min_samples_leaf\"]+2,1)] + [x for x in range(random_param_grid[\"min_samples_leaf\"], max(0,random_param_grid[\"min_samples_leaf\"]-2),-1)]))\n",
    "        param_grid[\"min_samples_split\"] = list(set([x for x in range(random_param_grid[\"min_samples_split\"], random_param_grid[\"min_samples_split\"]+4, 2)] + [x for x in range(random_param_grid[\"min_samples_split\"], max(1,random_param_grid[\"min_samples_split\"]-4), -2)]))\n",
    "        param_grid[\"max_features\"] = [random_param_grid[\"max_features\"]]\n",
    "        param_grid[\"learning_rate\"] = list(set([x for x in np.arange(random_param_grid[\"learning_rate\"],max(0,random_param_grid[\"learning_rate\"]-0.5),-0.02)]+[x for x in np.arange(random_param_grid[\"learning_rate\"],max(0,random_param_grid[\"learning_rate\"]+0.5),0.02)]))\n",
    "\n",
    "\n",
    "    else:\n",
    "        print(\"The shortcut is not recognized. Possible values are:\")\n",
    "        print(\"- rf for RandomForestClassifier()\")\n",
    "        print(\"- et for ExtraTreesClassifier()\")\n",
    "        print(\"- ab for AdaBoostClassifier()\")\n",
    "        print(\"- gb for GradientBoostingClassifier()\")\n",
    "        stage = \"stop\"\n",
    "    \n",
    "    # 2. Instantiating the grid search model\n",
    "    \n",
    "    if stage == \"go\":\n",
    "        \n",
    "        print(\"GridSearchCV is going to test the following parameters:\")\n",
    "        print(param_grid)\n",
    "        \n",
    "        grid_search = GridSearchCV(estimator = model, \n",
    "                                   param_grid = param_grid, \n",
    "                                   cv = 3, \n",
    "                                   n_jobs = -1, \n",
    "                                   verbose = 0, \n",
    "                                   scoring = \"accuracy\")\n",
    "\n",
    "\n",
    "    # 3. Fitting the grid search to the data\n",
    "    \n",
    "        grid_search.fit(X, y)\n",
    "    \n",
    "        print(\"\")\n",
    "        print(\"The best combination of parameters is:\")\n",
    "        print(grid_search.best_params_)\n",
    "        print(\"\")\n",
    "        print(\"With these parameters, we reach an accuracy score of {:0.2f}%.\" .format(grid_search.best_score_*100))\n",
    "    \n",
    "        return grid_search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Last, let's create a function that imbricates both functions above**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining a function that concatenates the two above\n",
    "\n",
    "def get_params(model, shortcut, param_grid, X, y):\n",
    "\n",
    "    # 1. Find best parameters using RandomizedSearchCV\n",
    "    \n",
    "    randomized_model = get_param_with_randomsearch(model, param_grid, X, y)\n",
    "    \n",
    "    param_grid = randomized_model.best_params_\n",
    "    \n",
    "    \n",
    "    # 2. Tune best parameters using GridSearchCV\n",
    "    \n",
    "    grid_model = get_param_with_gridsearchcv(model, shortcut, param_grid, X, y) \n",
    "    \n",
    "    return grid_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Let's now apply this function to each model.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "rf_model = get_params(RandomForestClassifier(), \"rf\", rf_grid, X, y)\n",
    "\n",
    "# Extra Trees\n",
    "et_model = get_params(ExtraTreesClassifier(), \"et\", et_grid, X, y)\n",
    "\n",
    "# AdaBoost\n",
    "ab_model = get_params(AdaBoostClassifier(), \"ab\", ab_grid, X, y)\n",
    "\n",
    "# GradientBoost\n",
    "gb_model = get_params(GradientBoostingClassifier(), \"gb\", gb_grid, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GradientBoostingClassifier(criterion='friedman_mse', init=None,\n",
       "              learning_rate=0.8600000000000002, loss='deviance',\n",
       "              max_depth=27, max_features='sqrt', max_leaf_nodes=None,\n",
       "              min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "              min_samples_leaf=1, min_samples_split=10,\n",
       "              min_weight_fraction_leaf=0.0, n_estimators=1000,\n",
       "              presort='auto', random_state=None, subsample=1.0, verbose=0,\n",
       "              warm_start=False)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# For saving time ...\n",
    "# ... storing best parameters\n",
    "\n",
    "rf_params = {'warm_start': True,\n",
    "             'verbose': 0,\n",
    "             'n_estimators': 600,\n",
    "             'min_samples_split': 8,\n",
    "             'min_samples_leaf': 4,\n",
    "             'max_features': 'sqrt',\n",
    "             'max_depth': 53}\n",
    "\n",
    "et_params = {'verbose': 0,\n",
    "             'n_jobs': -1,\n",
    "             'n_estimators': 800,\n",
    "             'min_samples_split': 8,\n",
    "             'min_samples_leaf': 3,\n",
    "             'max_features': 'sqrt',\n",
    "             'max_depth': 97}\n",
    "\n",
    "ab_params = {'n_estimators': 200,\n",
    "             'learning_rate': 0.019999999999999574}\n",
    "\n",
    "gb_params = {'n_estimators' : 1000,\n",
    "             'learning_rate': 0.8600000000000002,\n",
    "             'max_depth': 27,\n",
    "             'min_samples_leaf':1,\n",
    "             'min_samples_split': 10,\n",
    "             'max_features': 'sqrt'}\n",
    "\n",
    "\n",
    "# ... fitting models\n",
    "rf_model = RandomForestClassifier(**rf_params)\n",
    "rf_model.fit(X,y)\n",
    "\n",
    "et_model = RandomForestClassifier(**et_params)\n",
    "et_model.fit(X,y)\n",
    "\n",
    "ab_model = AdaBoostClassifier(**ab_params,algorithm='SAMME')\n",
    "ab_model.fit(X,y)\n",
    "\n",
    "gb_model = GradientBoostingClassifier(**gb_params)\n",
    "gb_model.fit(X,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_uuid": "d4dfc4a470f8b896b8befcc17877d99726d7fe9b"
   },
   "source": [
    "## 3. Predicting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_train = pd.DataFrame(\n",
    "                        {'PassengerId': train.PassengerId.tolist(),\n",
    "                         'rf_survived': rf_model.predict(X).tolist(),\n",
    "                         'et_survived': et_model.predict(X).tolist(),\n",
    "                         'ab_survived': ab_model.predict(X).tolist(),\n",
    "                         'gb_survived': gb_model.predict(X).tolist()\n",
    "                        })\n",
    "\n",
    "predictions_test = pd.DataFrame(\n",
    "                        {'PassengerId': test.PassengerId.tolist(),\n",
    "                         'rf_survived': rf_model.predict(test).tolist(),\n",
    "                         'et_survived': et_model.predict(test).tolist(),\n",
    "                         'ab_survived': ab_model.predict(test).tolist(),\n",
    "                         'gb_survived': gb_model.predict(test).tolist()\n",
    "                        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding the new columns to train and test df\n",
    "train = train.merge(predictions_train, how=\"left\", on=\"PassengerId\")\n",
    "\n",
    "test = test.merge(predictions_test, how=\"left\", on=\"PassengerId\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(\"Survived\", axis = 1)\n",
    "y = train.Survived"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second-Level Predictions from the First-level Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n",
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=sklearn.cross_validation.StratifiedKFold(labels=[0. 1. ... 1. 0.], n_folds=5, shuffle=True, random_state=None),\n",
       "       error_score='raise',\n",
       "       estimator=XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
       "       max_depth=3, min_child_weight=1, missing=None, n_estimators=100,\n",
       "       n_jobs=1, nthread=None, objective='binary:logistic', random_state=0,\n",
       "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
       "       silent=True, subsample=1),\n",
       "       fit_params=None, iid=True, n_jobs=5,\n",
       "       param_grid={'nthread': [4], 'objective': ['binary:logistic'], 'learning_rate': [0.05], 'max_depth': [6], 'min_child_weight': [11], 'silent': [1], 'subsample': [0.8], 'colsample_bytree': [0.7], 'n_estimators': [1000], 'seed': [1337]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_grid = {'nthread':[4],\n",
    "              'objective':['binary:logistic'],\n",
    "              'learning_rate': [0.05],\n",
    "              'max_depth': [6],\n",
    "              'min_child_weight': [11],\n",
    "              'silent': [1],\n",
    "              'subsample': [0.8],\n",
    "              'colsample_bytree': [0.7],\n",
    "              'n_estimators': [1000],\n",
    "              'seed': [1337]}\n",
    "\n",
    "\n",
    "xgb_model = GridSearchCV(xgboost.XGBClassifier(), xgb_grid, n_jobs=5, \n",
    "                   cv=StratifiedKFold(y, n_folds=5, shuffle=True), \n",
    "                   scoring='accuracy',\n",
    "                   verbose=0, refit=True)\n",
    "\n",
    "xgb_model.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "predictions = xgb_model.predict(test).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Formatting before uploading on Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Survived\n",
      "PassengerId          \n",
      "892                 0\n",
      "893                 0\n",
      "894                 0\n",
      "895                 0\n",
      "896                 0\n"
     ]
    }
   ],
   "source": [
    "submission = pd.DataFrame.from_csv('gender_submission.csv')\n",
    "submission['Survived'] = predictions\n",
    "print(submission.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 PassengerId\n",
      "892    0.0\n",
      "Name: Survived, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "submission_2 = test[[\"PassengerId\", \"ab_survived\"]].astype(int)\n",
    "submission_2.columns = [\"PassengerId\", \"Survived\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_2 = pd.DataFrame.from_csv('gender_submission.csv')\n",
    "submission_2.Survived= test.ab_survived.astype(int).as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_2.to_csv('submission2.csv', index=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
